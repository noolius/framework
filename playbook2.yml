---
  - name: Provisioning Alpine 3.14 K8s (all, post-first reboot)
    hosts: "*"
    become: true
    tasks:
      - name: Adding node IP to /e/hosts
        blockinfile:
          block: '{{ ansible_eth1.ipv4.address | quote }}   {{ ansible_hostname | quote }} {{ ansible_hostname | quote }}.local'
          path: /etc/hosts
      - name: Removing extraneous loopback entry from /e/hosts
        lineinfile:
          state: absent
          regexp: '^127\.0\.1\.1'
          path: /etc/hosts
  - name: Provisioning Alpine 3.14 K8s (control-planes, post-first reboot)
    hosts: control_planes
    become: true
    vars:
      apiserver: 192.168.133.10
      k8s_serviceport: 6443
      cni_chaining: |
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: cni-configuration
          namespace: kube-system
        data:
          cni-config: |-
            {
              "name": "generic-veth",
              "cniVersion": "0.3.1",
              "plugins": [
                {
                  "type": "calico",
                  "log_level": "info",
                  "datastore_type": "kubernetes",
                  "mtu": 1440,
                  "ipam": {
                      "type": "calico-ipam"
                  },
                  "policy": {
                      "type": "k8s"
                  },
                  "kubernetes": {
                      "kubeconfig": "/etc/cni/net.d/calico-kubeconfig"
                  }
                },
                {
                  "type": "portmap",
                  "snat": true,
                  "capabilities": {"portMappings": true}
                },
                {
                  "type": "cilium-cni"
                }
              ]
            }
    tasks:
      - name: Creating KUBECONFIG dir for root user
        file:
          path: /root/.kube
          state: directory
          owner: root
          group: root
          mode: '0750'
      - name: Creating KUBECONFIG dir for vagrant user
        file:
          path: /home/vagrant/.kube
          state: directory
          owner: vagrant
          group: vagrant
          mode: '0750'
      - name: Creating K8s manifests with api-server on internal (to-VMs) LAN
        ansible.builtin.shell: >
          nohup /usr/bin/kubeadm init --apiserver-advertise-address={{ apiserver | quote }}
          --pod-network-cidr=10.188.0.0/16 --cri-socket=/run/containerd/containerd.sock </dev/null >/vagrant/text 2>&1 &
        args:
          executable: /bin/bash
          warn: false
      - name: Waiting for /v/lib/kubelet/kubeadm-flags.env
        wait_for:
          path: /var/lib/kubelet/kubeadm-flags.env
          search_regex: pause
      - name: Replacing KUBELET_EXTRA_ARGS
        lineinfile:
          line: >
            KUBELET_KUBEADM_ARGS="--node-ip {{ ansible_eth1.ipv4.address | quote }} --container-runtime remote
            --container-runtime-endpoint=/run/containerd/containerd.sock --cgroup-driver=cgroupfs
            --pod-infra-container-image=k8s.gcr.io/pause:3.5"
          regexp: '^KUBELET_KUBEADM_ARGS='
          path: /var/lib/kubelet/kubeadm-flags.env
          backup: true
      - name: Creating /opt/cni/
        file:
          path: /opt/cni
          state: directory
          mode: '0755'
      - name: Creating /opt/cni/bin symlink for kubelet
        file:
          src: /usr/libexec/cni
          state: link
          dest: /opt/cni/bin
      - name: Restarting kubelet
        service:
          name: kubelet
          state: restarted
      - name: Waiting for /e/kubernetes/admin.conf
        wait_for:
          path: /etc/kubernetes/admin.conf
      - name: Copying KUBECONFIG for root user
        copy:
          src: /etc/kubernetes/admin.conf
          dest: /root/.kube/config
          owner: root
          group: root
          mode: '0400'
          remote_src: true
      - name: Copying KUBECONFIG for vagrant user
        copy:
          src: /etc/kubernetes/admin.conf
          dest: /home/vagrant/.kube/config
          owner: vagrant
          group: vagrant
          mode: '0400'
          remote_src: true
      - name: Adding Cilium's Helm charts
        ansible.builtin.command: /usr/local/bin/helm repo add cilium https://helm.cilium.io/
      - name: Updating Helm repos
        ansible.builtin.command: /usr/local/bin/helm repo update
      - name: Waiting for api-server to become available
        wait_for:
          port: '{{ k8s_serviceport | quote }}'
      - name: Waiting for (modified) Calico manifest
        wait_for:
          path: /vagrant/calico.yaml
          search_regex: 'CALICO_IPV4POOL_IPIP'
        register: got_calico_manifest
      - name: Deploying Calico
        ansible.builtin.command: /usr/bin/kubectl apply -f /vagrant/calico.yaml
        when: got_calico_manifest is succeeded
        register: deploy_calico
        ignore_errors: true
      - name: Re-deploying Calico due to missing kinds in ClusterRole*/PodDisruptionBudget
        ansible.builtin.shell: /usr/bin/kubectl delete -f /vagrant/calico.yaml ; /usr/bin/kubectl apply -f /vagrant/calico.yaml
        when: "'no matches for kind' in deploy_calico.stderr"
        args:
          warn: false
        register: redeploy_calico
      - name: Installing calicoctl as a pod
        ansible.builtin.command: /usr/bin/kubectl apply -f https://docs.projectcalico.org/manifests/calicoctl.yaml
        when: got_calico_manifest is succeeded
      - name: Copying CNI chaining for Cilium
        copy:
          content: '{{ cni_chaining }}'
          dest: /root/cni_chaining
          owner: root
          group: root
          mode: '0444'
          remote_src: true
      - name: Chaining Cilium on top of Calico
        ansible.builtin.command: /usr/bin/kubectl apply -f /root/cni_chaining
        when: deploy_calico is succeeded or redeploy_calico is succeeded
        register: chain_cilium
      - name: Deploying Cilium using its Helm chart
        ansible.builtin.command: >
          /usr/local/bin/helm install cilium cilium/cilium --version 1.10.3 --namespace kube-system
          --set k8sServiceHost={{ apiserver | quote }} --set k8sServicePort={{ k8s_serviceport | quote }}
          --set cni.chainingMode=generic-veth --set cni.customConf=true --set cni.configMap=cni-configuration
          --set tunnel=disabled --set enableIPv4Masquerade=false --set enableIdentityMark=false
          --set l7Proxy=false --set encryption.enabled=true --set encryption.type=wireguard
        when: chain_cilium is succeeded
      - name: Waiting for Cilium deployment to settle (grace 75s)
        pause:
          seconds: 75
        when: chain_cilium is succeeded
      - name: Restarting unmanaged pods
        ansible.builtin.shell: >
          /usr/bin/kubectl get po --all-namespaces
          -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,HOSTNETWORK:.spec.hostNetwork
          --no-headers=true | awk '/<none>/ {print "-n "$1" "$2}' | xargs -L 1 -r /usr/bin/kubectl delete po
        args:
          warn: false
          executable: /bin/bash
        when: chain_cilium is succeeded
  - name: Provisioning Alpine 3.14 K8s (workers, post-first reboot)
    hosts: workers
    become: true
    vars:
      apiserver: 192.168.133.10
      k8s_serviceport: 6443
    tasks:
      - name: Waiting for node registration token and hash
        wait_for:
          path: /vagrant/text
          search_regex: 'sha256'
        register: got_token_and_hash
      - name: Setting node registration token
        ansible.builtin.command: awk '/^kubeadm/ {print $5}' /vagrant/text
        when: got_token_and_hash is succeeded
        register: token
      - name: Setting node registration hash
        ansible.builtin.command: "awk -F: '/sha256/ {gsub(/ /, \"\", $0);print $2}' /vagrant/text"
        when: got_token_and_hash is succeeded
        register: ca_cert_hash
      - name: Joining node to cluster
        ansible.builtin.command: >
          /usr/bin/kubeadm join --token {{ token.stdout | quote }} {{ apiserver | quote }}:{{ k8s_serviceport | quote }}
          --discovery-token-ca-cert-hash sha256:{{ ca_cert_hash.stdout | quote }} --cri-socket=/run/containerd/containerd.sock
        when: got_token_and_hash is succeeded
      - name: Replacing KUBELET_EXTRA_ARGS
        lineinfile:
          line: >
            KUBELET_KUBEADM_ARGS="--node-ip {{ ansible_eth1.ipv4.address | quote }} --container-runtime remote
            --container-runtime-endpoint=/run/containerd/containerd.sock --cgroup-driver=cgroupfs
            --pod-infra-container-image=k8s.gcr.io/pause:3.5"
          regexp: '^KUBELET_KUBEADM_ARGS='
          path: /var/lib/kubelet/kubeadm-flags.env
          backup: true
        when: got_token_and_hash is succeeded
      - name: Creating /opt/cni/
        file:
          path: /opt/cni
          state: directory
          mode: '0755'
      - name: Creating /opt/cni/bin symlink for kubelet
        file:
          src: /usr/libexec/cni
          state: link
          dest: /opt/cni/bin
      - name: Silencing spurious kubelet log spam for (missing) /e/kubernetes/manifests/
        file:
          state: directory
          path: /etc/kubernetes/manifests
          owner: root
          group: root
          mode: '0700'
      - name: Restarting kubelet
        service:
          name: kubelet
          state: restarted
