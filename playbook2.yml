---
  - name: Provisioning Alpine 3.14 K8s (all, post-first reboot)
    hosts: "*"
    become: true
    tasks:
      - name: Adding node IP to /e/hosts
        blockinfile:
          block: '{{ ansible_eth1.ipv4.address | quote }}   {{ ansible_hostname | quote }} {{ ansible_hostname | quote }}.local'
          path: /etc/hosts
      - name: Removing extraneous loopback entry from /e/hosts
        lineinfile:
          state: absent
          regexp: '^127\.0\.1\.1'
          path: /etc/hosts
  - name: Provisioning Alpine 3.14 K8s (control-planes, post-first reboot)
    hosts: control_planes
    become: true
    vars:
      apiserver: 192.168.133.10
      k8s_serviceport: 6443
      storageclass_manifest: |
        apiVersion: storage.k8s.io/v1
        kind: StorageClass
        metadata:
          name: local
        provisioner: kubernetes.io/no-provisioner
        volumeBindingMode: WaitForFirstConsumer
      bash_profile: |
        alias k='kubectl'
        alias ns='kubens'
        alias ctx='kubectx'
      persistentvolume1_manifest: |
        apiVersion: v1
        kind: PersistentVolume
        metadata:
          name: pv-1
        spec:
          capacity:
            storage: 10Gi
          volumeMode: Filesystem
          accessModes:
          - ReadWriteOnce
          persistentVolumeReclaimPolicy: Delete
          local:
            path: /srv
          nodeAffinity:
            required:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                  - node2
                  - node3
                  - node4
      persistentvolume2_manifest: |
        apiVersion: v1
        kind: PersistentVolume
        metadata:
          name: pv-2
        spec:
          capacity:
            storage: 10Gi
          volumeMode: Filesystem
          accessModes:
          - ReadWriteOnce
          persistentVolumeReclaimPolicy: Delete
          local:
            path: /srv
          nodeAffinity:
            required:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                  - node2
                  - node3
                  - node4
      calico_manifest_diff: |
        --- calico.yaml 2021-08-08 12:00:17.582096536 -0400
        +++ calico.yaml 2021-08-30 23:59:21.350456483 -0400
        @@ -3848,12 +3848,14 @@
                     # Auto-detect the BGP IP address.
                     - name: IP
                       value: "autodetect"
        +            - name: IP_AUTODETECTION_METHOD
        +              value: "interface=eth1"
                     # Enable IPIP
                     - name: CALICO_IPV4POOL_IPIP
        -              value: "Always"
        +              value: "Never"
                     # Enable or Disable VXLAN on the default IP pool.
                     - name: CALICO_IPV4POOL_VXLAN
        -              value: "Never"
        +              value: "Always"
                     # Set MTU for tunnel device used if ipip is enabled
                     - name: FELIX_IPINIPMTU
                       valueFrom:
        @@ -3898,7 +3900,7 @@
                       command:
                       - /bin/calico-node
                       - -felix-live
        -              - -bird-live
        +                #    - -bird-live
                     periodSeconds: 10
                     initialDelaySeconds: 10
                     failureThreshold: 6
        @@ -3908,7 +3910,7 @@
                       command:
                       - /bin/calico-node
                       - -felix-ready
        -              - -bird-ready
        +                #  - -bird-ready
                     periodSeconds: 10
                     timeoutSeconds: 10
                   volumeMounts:
      cni_chaining: |
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: cni-configuration
          namespace: kube-system
        data:
          cni-config: |-
            {
              "name": "generic-veth",
              "cniVersion": "0.4.0",
              "plugins": [
                {
                  "type": "calico",
                  "log_level": "info",
                  "datastore_type": "kubernetes",
                  "mtu": 1440,
                  "ipam": {
                      "type": "calico-ipam"
                  },
                  "policy": {
                      "type": "k8s"
                  },
                  "kubernetes": {
                      "kubeconfig": "/etc/cni/net.d/calico-kubeconfig"
                  }
                },
                {
                  "type": "portmap",
                  "snat": true,
                  "capabilities": {"portMappings": true}
                },
                {
                  "type": "cilium-cni"
                }
              ]
            }
    tasks:
      - name: Creating KUBECONFIG dir for root user
        file:
          path: /root/.kube
          state: directory
          owner: root
          group: root
          mode: '0750'
      - name: Creating KUBECONFIG dir for vagrant user
        file:
          path: /home/vagrant/.kube
          state: directory
          owner: vagrant
          group: vagrant
          mode: '0750'
      - name: Checking whether to create K8s infra
        ansible.builtin.stat:
          path: /vagrant/text
        register: existing_k8s_token
      - name: Creating K8s manifests with api-server on internal (to-VMs) LAN
        ansible.builtin.shell: >
          nohup /usr/bin/kubeadm init --apiserver-advertise-address={{ apiserver | quote }}
          --pod-network-cidr=10.188.0.0/16 --cri-socket=/run/containerd/containerd.sock </dev/null >/vagrant/text 2>&1 &
        args:
          executable: /bin/bash
          warn: false
        when: not existing_k8s_token.stat.exists
        register: create_k8s_infra
      - name: Waiting for /v/lib/kubelet/kubeadm-flags.env
        wait_for:
          path: /var/lib/kubelet/kubeadm-flags.env
          search_regex: pause
        when: create_k8s_infra is succeeded
        register: kubeadm_flags_done
      - name: Replacing KUBELET_EXTRA_ARGS
        lineinfile:
          line: >
            KUBELET_KUBEADM_ARGS="--node-ip {{ ansible_eth1.ipv4.address | quote }} --container-runtime remote
            --container-runtime-endpoint=/run/containerd/containerd.sock --cgroup-driver=cgroupfs
            --pod-infra-container-image=k8s.gcr.io/pause:3.5"
          regexp: '^KUBELET_KUBEADM_ARGS='
          path: /var/lib/kubelet/kubeadm-flags.env
          backup: true
        when: kubeadm_flags_done is succeeded
        register: kubelet_extra_args
      - name: Creating /opt/cni/
        file:
          path: /opt/cni
          state: directory
          mode: '0755'
        when: kubelet_extra_args is succeeded
      - name: Creating /opt/cni/bin symlink for kubelet
        file:
          src: /usr/libexec/cni
          state: link
          dest: /opt/cni/bin
        when: kubelet_extra_args is succeeded
      - name: Restarting kubelet
        service:
          name: kubelet
          state: restarted
        when: kubelet_extra_args is succeeded
        register: kubelet_restarted
      - name: Waiting for /e/kubernetes/admin.conf
        wait_for:
          path: /etc/kubernetes/admin.conf
        when: kubelet_restarted is succeeded
        register: adminconf
      - name: Copying KUBECONFIG for root user
        copy:
          src: /etc/kubernetes/admin.conf
          dest: /root/.kube/config
          owner: root
          group: root
          mode: '0600'
          remote_src: true
        when: adminconf is succeeded
        register: root_config
      - name: Populating kube* aliases in ~root/.bash_profile
        copy:
          content: '{{ bash_profile }}'
          dest: /root/.bash_profile
          owner: root
          group: root
          mode: '0600'
          remote_src: true
      - name: Copying KUBECONFIG for vagrant user
        copy:
          src: /etc/kubernetes/admin.conf
          dest: /home/vagrant/.kube/config
          owner: vagrant
          group: vagrant
          mode: '0600'
          remote_src: true
        when: adminconf is succeeded
      - name: Populating kube* aliases in ~vagrant/.bash_profile
        copy:
          content: '{{ bash_profile }}'
          dest: /home/vagrant/.bash_profile
          owner: vagrant
          group: vagrant
          mode: '0600'
          remote_src: true
      - name: Writing StorageClass manifest
        copy:
          content: '{{ storageclass_manifest }}'
          dest: /root/sc_manif
          owner: root
          group: root
          mode: '0444'
          remote_src: true
      - name: Writing PersistentVolume 1 manifest
        copy:
          content: '{{ persistentvolume1_manifest }}'
          dest: /root/pv1_manif
          owner: root
          group: root
          mode: '0444'
          remote_src: true
      - name: Writing PersistentVolume 2 manifest
        copy:
          content: '{{ persistentvolume2_manifest }}'
          dest: /root/pv2_manif
          owner: root
          group: root
          mode: '0444'
          remote_src: true
      - name: Adding Cilium's Helm charts
        ansible.builtin.command: /usr/local/bin/helm repo add cilium https://helm.cilium.io/
        when: root_config is succeeded
      - name: Adding Crossplane's Helm charts
        ansible.builtin.command: /usr/local/bin/helm repo add crossplane-stable https://charts.crossplane.io/stable
        when: root_config is succeeded
      - name: Updating Helm repos
        ansible.builtin.command: /usr/local/bin/helm repo update
        when: root_config is succeeded
      - name: Waiting for api-server to become available
        wait_for:
          port: '{{ k8s_serviceport | quote }}'
        register: apiserver_avail
      - name: Downloading Calico networking manifest
        get_url:
          url: https://docs.projectcalico.org/archive/v3.21/manifests/calico.yaml
          dest: /root/calico.yaml
          checksum: sha256:980eb9d23491a23e936868fbcdfdbff5cf7ffa606a3c6fa3a53020823f5440ec
      - name: Writing unified GNU diff of Calico networking manifest
        copy:
          content: '{{ calico_manifest_diff }}'
          dest: /root/calico.yaml.diff
          owner: root
          group: root
          mode: '0444'
          remote_src: true
      - name: Modifying Calico manifest
        ansible.posix.patch:
          src: /root/calico.yaml.diff
          dest: /root/calico.yaml
          remote_src: true
        register: got_calico_manifest
      - name: Deploying Calico
        ansible.builtin.command: /usr/bin/kubectl apply -f /root/calico.yaml
        when: got_calico_manifest is succeeded and apiserver_avail is succeeded
        register: deploy_calico
        ignore_errors: true
      - name: Re-deploying Calico due to missing kinds in ClusterRole*/PodDisruptionBudget
        ansible.builtin.shell: /usr/bin/kubectl delete -f /root/calico.yaml ; /usr/bin/kubectl apply -f /root/calico.yaml
        when: "'no matches for kind' in deploy_calico.stderr"
        args:
          warn: false
        register: redeploy_calico
      - name: Installing calicoctl on control-plane
        get_url:
          url: https://github.com/projectcalico/calicoctl/releases/download/v3.21.0/calicoctl-linux-amd64
          dest: /usr/local/bin/calicoctl
          owner: root
          group: root
          mode: '0555'
          checksum: sha256:7533b33d98c8e3f035a22537c7885c1bba64dcb8107fd6bb83286dc6319bbd59
        when: got_calico_manifest is succeeded
      - name: Updating Calico IP autodetection
        ansible.builtin.command: /usr/bin/kubectl set env ds/calico-node -n kube-system IP_AUTODETECTION_METHOD=interface=eth1
        when: deploy_calico is succeeded or redeploy_calico is succeeded
        register: update_calico_ip_autodetection_pre_wg
      - name: Waiting for Calico to settle (grace 59s)
        pause:
          seconds: 59
      - name: Applying WireGuard(tm) enciphering for inter-node traffic
        ansible.builtin.command: /usr/local/bin/calicoctl patch felixconfiguration default --type='merge' -p '{"spec":{"wireguardEnabled":true}}'
        when: update_calico_ip_autodetection_pre_wg is succeeded
        register: apply_wg
      - name: Patching ConfigMap to modify MTU (-> 1440) for cluster traffic
        ansible.builtin.command: /usr/bin/kubectl patch cm/calico-config -n kube-system --type merge -p '{"data":{"veth_mtu":"1440"}}'
        when: apply_wg is succeeded
        register: modify_mtu_post_wg
      - name: Redeploying Calico pods with ConfigMap change
        ansible.builtin.command: /usr/bin/kubectl rollout restart ds/calico-node -n kube-system
        when: modify_mtu_post_wg is succeeded
        register: update_calico_ip_autodetection
      - name: Copying CNI chaining for Cilium
        copy:
          content: '{{ cni_chaining }}'
          dest: /root/cni_chaining
          owner: root
          group: root
          mode: '0444'
          remote_src: true
      - name: Chaining Cilium on top of Calico
        ansible.builtin.command: /usr/bin/kubectl apply -f /root/cni_chaining
        when: update_calico_ip_autodetection is succeeded
        register: chain_cilium
      - name: Deploying Cilium using its Helm chart
        ansible.builtin.command: >
          /usr/local/bin/helm install cilium cilium/cilium --version 1.10.5 --namespace kube-system
          --set k8sServiceHost={{ apiserver | quote }} --set k8sServicePort={{ k8s_serviceport | quote }}
          --set cni.chainingMode=generic-veth --set cni.customConf=true --set cni.configMap=cni-configuration
          --set tunnel=disabled --set enableIPv4Masquerade=false --set enableIdentityMark=false
          --set hubble.relay.enabled=true --set hubble.ui.enabled=true
        when: chain_cilium is succeeded
        register: deploy_cilium
      - name: Waiting for Cilium deployment to settle (grace 35s)
        pause:
          seconds: 35
        when: deploy_cilium is succeeded
      - name: Waiting for /u/libexec/cni/calico*
        wait_for:
          path: /usr/libexec/cni/calico
        when: update_calico_ip_autodetection is succeeded
        register: old_calico_cni_plugin_exists
      - name: Checking whether to copy precompiled Calico CNI plugins (Go/musl)
        ansible.builtin.stat:
          path: /vagrant/calico
        when: old_calico_cni_plugin_exists is succeeded
        register: precomplied_calico_cni_plugin
      - name: Copying precompiled Calico CNI plugins (Go/musl)
        copy:
          src: /vagrant/calico
          dest: "{{ item }}"
          owner: root
          group: root
          mode: '4755'
          remote_src: true
          checksum: 5189becde8caf857f79ded2aa87b2dfbb6ce8ef8
        when: precomplied_calico_cni_plugin.stat.exists
        loop:
          - /usr/libexec/cni/calico
          - /usr/libexec/cni/calico-ipam
      - name: Running Linkerd pre-install check
        ansible.builtin.command: /usr/local/bin/linkerd check --pre
        register: precheck_linkerd
        when: deploy_cilium is succeeded
      - name: Installing Linkerd config
        ansible.builtin.shell: >
          set -o pipefail && /usr/local/bin/linkerd install config | /usr/bin/kubectl apply -f -
        args:
          warn: false
          executable: /bin/bash
        when: precheck_linkerd.rc == 0
        register: installing_config_stage_linkerd
      - name: Validating Linkerd config
        ansible.builtin.command: /usr/local/bin/linkerd check config
        register: validating_config_linkerd
        when: installing_config_stage_linkerd is succeeded
      - name: Creating namespaces for linkerd-viz, k-rail & Crossplane
        ansible.builtin.command: /usr/bin/kubectl create ns "{{ item }}"
        loop:
          - linkerd-viz
          - crossplane-system
          - k-rail
        when: validating_config_linkerd.rc == 0
        register: created_ns_k_rail_crossplane
      - name: Annotating k-rail & Crossplane namespaces for Linkerd2 auto-injection
        ansible.builtin.command: /usr/bin/kubectl annotate ns "{{ item }}" linkerd.io/inject=enabled
        loop:
          - crossplane-system
          - k-rail
        when: created_ns_k_rail_crossplane is succeeded
      - name: Defensively labeling kube-system, k-rail, linkerd* & crossplane namespaces not to break functionality
        ansible.builtin.command: /usr/bin/kubectl label namespace "{{ item }}" k-rail/ignore=true
        loop:
          - kube-system
          - k-rail
          - linkerd
          - linkerd-viz
          - crossplane-system
        when: created_ns_k_rail_crossplane is succeeded and validating_config_linkerd.rc == 0
        register: defensively_labeled_ns
      - name: Installing Linkerd control-plane
        ansible.builtin.shell: >
          set -o pipefail && /usr/local/bin/linkerd install --ha control-plane | /usr/bin/kubectl apply -f -
        args:
          warn: false
          executable: /bin/bash
        when: defensively_labeled_ns is succeeded
        register: installing_control_plane_linkerd
      - name: Waiting for Linkerd deployment to settle (grace 90s)
        pause:
          seconds: 90
        when: installing_control_plane_linkerd.rc == 0
      - name: Validating Linkerd control-plane
        ansible.builtin.command: /usr/local/bin/linkerd check
        when: installing_control_plane_linkerd.rc == 0
        register: ok_linkerd_check
      - name: Deploying Crossplane using its Helm chart
        ansible.builtin.command: /usr/local/bin/helm install crossplane --namespace crossplane-system crossplane-stable/crossplane
        when: installing_control_plane_linkerd.rc == 0
      - name: Registering local StorageClass and PersistentVolumes
        ansible.builtin.command: /usr/bin/kubectl create -f "{{ item }}"
        when: apiserver_avail is succeeded
        loop:
          - /root/sc_manif
          - /root/pv1_manif
          - /root/pv2_manif
      - name: Adding k-rail's Helm charts
        ansible.builtin.command: /usr/local/bin/helm repo add k-rail https://cruise-automation.github.io/k-rail/
        when: root_config is succeeded
        register: add_k_rail_helm
      - name: Updating Helm repos
        ansible.builtin.command: /usr/local/bin/helm repo update
        when: add_k_rail_helm.rc == 0
        register: updated_helm_repos
      - name: Deploying k-rail using its Helm chart
        ansible.builtin.command: /usr/local/bin/helm install k-rail k-rail/k-rail --namespace k-rail
        when: updated_helm_repos.rc == 0
  - name: Provisioning Alpine 3.14 K8s (workers, post-first reboot)
    hosts: workers
    become: true
    vars:
      apiserver: 192.168.133.10
      k8s_serviceport: 6443
    tasks:
      - name: Setting /srv 01777 for use by PVs
        file:
          path: /srv
          state: directory
          owner: root
          group: root
          mode: '01777'
      - name: Waiting for node registration token and hash
        wait_for:
          path: /vagrant/text
          search_regex: 'sha256'
        register: got_token_and_hash
      - name: Setting node registration token
        ansible.builtin.command: awk '/^kubeadm/ {print $5}' /vagrant/text
        when: got_token_and_hash is succeeded
        register: token
      - name: Setting node registration hash
        ansible.builtin.command: "awk -F: '/sha256/ {gsub(/ /, \"\", $0);print $2}' /vagrant/text"
        when: got_token_and_hash is succeeded
        register: ca_cert_hash
      - name: Joining node to cluster
        ansible.builtin.command: >
          /usr/bin/kubeadm join --token {{ token.stdout | quote }} {{ apiserver | quote }}:{{ k8s_serviceport | quote }}
          --discovery-token-ca-cert-hash sha256:{{ ca_cert_hash.stdout | quote }} --cri-socket=/run/containerd/containerd.sock
        when: got_token_and_hash is succeeded
      - name: Replacing KUBELET_EXTRA_ARGS
        lineinfile:
          line: >
            KUBELET_KUBEADM_ARGS="--node-ip {{ ansible_eth1.ipv4.address | quote }} --container-runtime remote
            --container-runtime-endpoint=/run/containerd/containerd.sock --cgroup-driver=cgroupfs
            --pod-infra-container-image=k8s.gcr.io/pause:3.5"
          regexp: '^KUBELET_KUBEADM_ARGS='
          path: /var/lib/kubelet/kubeadm-flags.env
          backup: true
        when: got_token_and_hash is succeeded
      - name: Creating /opt/cni/
        file:
          path: /opt/cni
          state: directory
          mode: '0755'
      - name: Creating /opt/cni/bin symlink for kubelet
        file:
          src: /usr/libexec/cni
          state: link
          path: /opt/cni/bin
          force: true
      - name: Silencing spurious kubelet log spam for (missing) /e/kubernetes/manifests/
        file:
          state: directory
          path: /etc/kubernetes/manifests
          owner: root
          group: root
          mode: '0700'
      - name: Restarting kubelet
        service:
          name: kubelet
          state: restarted
      - name: Waiting for /u/libexec/cni/calico*
        wait_for:
          path: /usr/libexec/cni/calico
        register: old_calico_cni_plugin_exists
      - name: Checking whether to copy precompiled Calico CNI plugins (Go/musl)
        ansible.builtin.stat:
          path: /vagrant/calico
        when: old_calico_cni_plugin_exists is succeeded
        register: precomplied_calico_cni_plugin
      - name: Copying precompiled Calico CNI plugins (Go/musl)
        copy:
          src: /vagrant/calico
          dest: "{{ item }}"
          owner: root
          group: root
          mode: '4755'
          remote_src: true
          checksum: 5189becde8caf857f79ded2aa87b2dfbb6ce8ef8
        when: precomplied_calico_cni_plugin.stat.exists
        loop:
          - /usr/libexec/cni/calico
          - /usr/libexec/cni/calico-ipam
